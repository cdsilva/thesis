
\chapter{Introduction\label{ch:intro}}

Over the past decade, the amount of available data has grown exponentially in many, if not all, scientific diciplines. 
%
Computational speed has increased (according to Moore's law), allowing for larger and longer simulations, and
the cost of memory has decreased dramatically, such that data storage is no longer a bottleneck for most applications. 
%
The Internet has allowed for the rapid sharing and exporation of new data sets, such that data from many groups and collaborators can be combined without any face-to-face interaction. 
%
All of these advances have ushered us into the era of ``Big Data'', where the challenge is not obtaining the data, but rather, searching for the relevant trends and patterns within this seemingly endless sea of information.

The prototypical examples of data science problems arise in consumer-driven applications. %
Tasks such as filtering email spam, predicting what a specific customer would like to purchase online, and extracting trends from Twitter and Facebook data have all led to interesting and novel data analysis algorithms. 
%
However, the natural sciences and engineering have also witnessed a dramatic growth in the amount of available data, and data-driven algorithms have the potential to contribute to these fields as well.

Machine learning tasks can be broadly divided into two categories: supervised and unsupervised. 
%
In supervised learning, data consists of inputs and outputs, where each training data point consists of an input vector and an output response. 
%
In contrast, unsupervised learning operates on unlabeled data, where the task is to uncover structure and patterns {\em within} the data set. 
%
We will focus on this second task of unsupervised learning. 
%
More specifically, we will focus on dimensionality reduction, where the goal is to reduce the dimensionality of the data while still retaining most of the relevant patterns and structure within the data set. 
%
Reducing the dimensionality of a data set to two or three dimensions allows for easy visualization of the data set. 
%
Furthermore, reducing the dimensionality makes computations, such as integrating a system of differential equations or fitting a model, easier and faster. 


%
%
%TODO: this is from JCP intro....
%
%The last decade has witnessed extensive advances in dimensionality reduction techniques:
%finding meaningful low-dimensional descriptions of high-dimensional data \cite{tenenbaum2000global,roweis2000nonlinear,Donoho2003,Belkin2003,Coifman2006}.
%
%Often, high-dimensional data do not uniformly fill the entire ambient space, but rather lie on a lower-dimensional manifold.
%
%Discovering a parameterization of the manifold therefore yields a compact representation of the data, and may lead to efficient analysis and processing schemes.
%
%In particular, these developments have the potential to significantly enable the computational exploration
%of physicochemical problems.
%
TODO: change slightly from JCP....
Traditionally, model reduction in engineering disciplines has been done using some {\em a priori} knowledge about separation of time scales,
such as using the quasi-steady state approximation to reduce a network of chemical reactions \cite{bowen1963singular}.
%
Alternatively, empirical knowledge about the system of interest has been used to develop coarse grained descriptions,
such as grouping atoms of a macromolecule into larger ``beads'' to accelerate simulations\cite{monticelli2008martini, spiga2013electrostatic,izvekov2005systematic, saunders2013coarse}.
%
However, the use of manifold learning allows for the systematic discovery of data-driven, low-dimensional descriptions that circumvent the need for such {\em a priori} knowledge of the system.
%
If the (high-dimensional) data arise from, for example, a
molecular dynamics simulation of a macromolecule in solution or from the stochastic
simulation of a complex chemical reaction scheme, the detection of a few good, coarse-grained
``reduction coordinates" can be invaluable in understanding and predicting system behavior.
%
%For example, such reduction coordinates may be the dihedral angles inferred from trajectories of the atoms of a macromolecule.


\section{Background on dimensionality reduction and manifold learning} \label{sec:background}


For the past century, principal component analysis (PCA) \cite{shlens2005tutorial} has been the standard dimensionality reduction technique.
%
PCA projects (high-dimensional) data onto a lower-dimensional hyperplane, where the hyperplane is chosen to maximize the variance of the projected data. 
%
The vectors which define this hyperplane are the eigenvectors of a covariance matrix estimated from the data. 
%
PCA is extraordinarily simple to implement, and fast and accurate algorithms already exist for eigenvalue decomposition of a matrix, which is the main computation.
%
PCA has therefore been used in a variety of disciplines, including signal processing \cite{...},  fluid dynamics \cite{...}, and genetics \cite{...}. 

Although PCA has been extremely useful in a variety of contexts and applications, its functionality is limited because it is a linear technique. 
%
The past twenty years have seen increased interest in algorithms which can accommodate and uncover nonlinear structure in data.
%
Kernel principal components analysis \cite{scholkopf1997kernel} uses the ``kernel trick'' to do traditional PCA in a higher-dimensional, more complex basis which can capture nonlinearities. 
%
Local linear embedding \cite{roweis2000nonlinear} does PCA {\em locally} on patches of the data. 
%
Isomap \cite{tenenbaum2000global} first constructs a nearest-neighbor graph on the data, and then uses multidimensional scaling \cite{joseph1978multidimensional} to embed the data, where the distance between a pair of data points is given by the length of the shortest path between them in the graph. 

Simultaneously, there was renewed interest in algorithms for analyzing and extracting structure in graphs, due to the prevalence of network, internet, and social media data. 
%
The graph Laplacian had been used to analyze many such data sets \cite{shi2000normalized, ng2002spectral}. 
%
It was noted approximately a decade ago that, given a network constructed on the data (similar to the first step in the Isomap algorithm), the graph Laplacian could be used to analyze this network. 
%
The eigenvectors of this Laplacian then yield coordinates which parameterize the nodes of the network (the data points). 
%
This algorithm, called Laplacian Eigenmaps, has both practical advantages (it is both simple to implement and robust to noise) as well as elegant theoretical guarantees (the discrete operator approaches the Laplacian on the underlying manifold assuming the data are uniformly sampled) \cite{Belkin2003}. 
%
Soon after Laplacian Eigenmaps, the {\em diffusion maps} algorithm was proposed as a generalization \cite{coifman2005geometric, coifman2006geometric}. 
%
This algorithm yields a family of embeddings, and provides theoretical understanding for the resulting embedding even when the data are nonuniformly sampled from the underlying manifold. 


\section{Outline of this dissertation}

The focus of this thesis is adapting and utilizing data-driven algorithms to address engineering problems. 
%
The core of each analysis will be the same data-driven nonlinear manifold learning algorithm (diffusion maps \citep{Coifman2006}). 
%
However, each problem will come with its own set of unique considerations and constraints, which must be addressed for accurate and informative analysis. 

In \chap~\ref{ch:math}, we outline the mathematical techniques common throughout this thesis work and introduce some notation. 
%
\chap~\ref{ch:multiscale} the discusses using data-driven techniques for the analysis of multiscale stochastic data. 
%
Here, we show that, by introducing a more sophisticated metric, we can uncover a parameterization of data which is invariant to fast stochastic fluctuations, even when these fluctuations are large. 
%
We present both theoretical analysis for our proposed approach, as well as demonstrate the approach on synthetic data sets. 
%
\chap~\ref{ch:merging} then discusses using data-driven techniques to merge data from different observations of the same underlying system. 
%
In this case, we also use a more sophisticated metric which removes the effect of nonlinear observation/measurement functions. 
%
Different observations of the same underlying system are therefore merged into a common framework. 
%
We demonstrate this approach on two simulation data sets: one is a simulation of a chemical reaction network, and the other is a molecular dynamics simulation of a small peptide fragment.
%
\chap~\ref{ch:harmonics} discusses the key issue of harmonics or ``repeated eigendirections'' when using manifold learning techniques, where several modes can parmaeterize the same direction in the data set. 
%
We present an algorithm for automatically detecting these repeated eigendirctions; this algorithm, based on local linear regression, is both fast and general, and allows us factor out the repeated eigendirections to uncover the true dimensionality of the underlying manifold and yield the most parsimonious representation of the data. 
%
\chap~\ref{ch:drosophila} then presents an application, where we used vector diffusion maps to both register and temporally order imaging data sets in developmental biology. 
% 
This thesis is therefore an exploration of different adaptations and extensions of traditional manifold learning techniques. 
%
These extensions allow us to analyze data from engineering dynamical systems. 
