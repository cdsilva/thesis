% !TEX root = ../thesis.tex


\chapter{Introduction\label{ch:intro}}

Modeling dynamical systems is an essential component of most science and engineering disciplines.
%
Substantial research efforts are dedicated to constructing models for complex phenomena, such as chemical reactions \cite{dong2007simplification, gillespie1977exact, gallagher1986combined}, fluid flows \cite{anderson1995computational}, and gene expression profiles \cite{bar2004analyzing, storey2005significance}.
%
These models allow researchers to run very inexpensive {\em in silico} ``experiments'', and can also be incorporated into larger systems-level models and optimization routines \cite{daoutidis2013engineering, oluwole2006rigorous, rubert2014advanced}.

Often a lower-dimensional model which captures the general trends and macroscale behavior of a dynamical system is preferable to a higher-dimensional model which captures the fine-scale details.
%
Lower-dimensional models are simple and often easy to interpret.
%
Furthermore, lower-dimensional models are computationally less expensive and can more easily be incorporated into larger modeling frameworks.
%
However, deciding on how many and what variables are required to (approximately) describe the system dynamics is a difficult task with no clear solution.


Traditionally, model reduction in engineering disciplines has been done using some {\em a priori} knowledge about separation of time scales,
such as using the quasi-steady state approximation to reduce a network of chemical reactions \cite{bowen1963singular}.
%
Alternatively, empirical knowledge about the system of interest has been used to develop coarse grained descriptions, such as grouping atoms of a macromolecule into larger ``beads'' to accelerate simulations \cite{izvekov2005systematic, monticelli2008martini, saunders2013coarse, spiga2013electrostatic}.
%
Progress variables and order parameters which effectively parameterize a system's dynamics, such as the radius of gyration in protein folding problems \cite{lazaridis1997new, kim2015systematic} or a  morphological feature which parameterizes an organism's developmental progress \cite{dubuis2013accurate, hamaratoglu2011dpp}, are often posited after many empirical observations and are not immediately obvious for new systems of interest.
%
However, dimensionality reduction algorithms allow for the systematic discovery of data-driven, low-dimensional descriptions that circumvent the need for such {\em a priori} knowledge, and the automated detection of appropriate order parameters can be crucial for understanding and modeling system behavior.
%
%For example, such reduction coordinates may be the dihedral angles inferred from trajectories of the atoms of a macromolecule.
%
Data from dynamical systems often comes in one of two forms.
%
Data can be collected directly from experiments for which no model exists.
%
Alternatively, data can be collected from simulations of a detailed, high-dimensional model at the microscopic level; in this case, the task is finding a lower-dimensional model which captures the macroscopic dynamics.
%
We will discuss both types of data in this dissertation.

\section{Background on Dimensionality Reduction} \label{sec:background}


For the past century, principal component analysis (PCA) \cite{shlens2005tutorial} has been the standard data-driven dimensionality reduction technique.
%
PCA projects (high-dimensional) data onto a lower-dimensional hyperplane, where the hyperplane is chosen to maximize the variance of the projected data.
%
The vectors which define this hyperplane are the eigenvectors of a covariance matrix estimated from the data.
%
PCA is extraordinarily simple to implement, and fast and accurate algorithms already exist for eigenvalue decomposition of a matrix, the main computation in PCA.
%
PCA has therefore been used in a variety of disciplines, including fluid dynamics \cite{rowley2004model, kunisch2002galerkin}, genetics \cite{alter2000singular, troyanskaya2001missing}, and signal processing \cite{vaseghi2008advanced}.

Although PCA has been extremely useful in a variety of contexts and applications, its functionality is limited because it is a linear technique.
%
The past twenty years have witnessed increased interest in algorithms which can accommodate and uncover nonlinear structure in data.
%
Kernel principal components analysis \cite{scholkopf1997kernel} uses the ``kernel trick'' to perform PCA in a higher-dimensional, more complex basis which can capture nonlinearities.
%
Local linear embedding \cite{roweis2000nonlinear} does PCA {\em locally} on patches of the data, and then merges these linear patches together to obtain a global parameterization which can reveal nonlinear structure in the data.
%
Isomap \cite{tenenbaum2000global} first constructs a weighted graph on the data in which nearby data points are connected by stronger edges.
%
The algorithm then uses multidimensional scaling \cite{joseph1978multidimensional} to embed the data in a space where the Euclidean distance between a pair of data points is (approximately) length of the shortest path between the pair in the graph.
%
This graph-based distance attempts to approximate the geodesic distance along the underlying manifold on which the data are presumed to lie.

Simultaneously, there was renewed interest in algorithms for analyzing and extracting structure in graphs, due to the prevalence of network, internet, and social media data.
%
The graph Laplacian had been used to analyze many such data sets \cite{shi2000normalized, ng2002spectral}.
%
It was noted approximately a decade ago that, given a network constructed on the data (similar to the first step in the Isomap algorithm), the graph Laplacian could be used to analyze this network.
%
The eigenvectors of this Laplacian then yield coordinates which parameterize the nodes of the network (the data points).
%
This algorithm, called Laplacian Eigenmaps, has both practical advantages (it is both simple to implement and robust to noise) as well as elegant theoretical guarantees (the discrete operator approaches the Laplacian on the underlying manifold assuming the data are uniformly sampled) \cite{Belkin2003}.
%
Soon after Laplacian Eigenmaps, the diffusion maps algorithm was proposed as a generalization \cite{coifman2005geometric, coifman2006geometric}.
%
This algorithm yields a family of embeddings, and provides theoretical understanding for the resulting embedding even when the data are nonuniformly sampled from the underlying manifold.


\section{Outline of Dissertation}

The focus of this thesis is using data mining algorithms to extract reduced parameterizations of data from dynamical systems.
%
The core of each analysis will be diffusion maps \citep{Coifman2006}, a data-driven nonlinear manifold learning algorithm.
%
However, each problem will come with its own set of unique considerations and constraints which must be addressed for accurate and informative analysis.

In \chap~\ref{ch:math}, we outline the mathematical techniques common throughout this thesis work and introduce some notation.
%
\chap~\ref{ch:multiscale} discusses using data-driven techniques for the analysis of multiscale stochastic data.
%
By using the Mahalanobis distance \cite{mahalanobis1936generalized}, rather than the standard Euclidean distance, we uncover a parameterization of data which respects the underlying slow dynamical modes and is invariant to fast stochastic fluctuations, even when these fluctuations are large.
%
We both present a theoretical analysis for our proposed approach, as well as illustrate the approach using synthetic data sets.
%
\chap~\ref{ch:merging} then discusses extracting a parameterization of data which is invariant to the specific measurement function used to observe the underlying system.
%
In this case, we use the Mahalanobis distance to remove the effect of nonlinear observation/measurement functions, so that different observations of the same dynamical system can be merged into a common framework.
%
We demonstrate this approach on two simulation data sets: (1) a simulation of a chemical reaction network, and (2) a molecular dynamics simulation of a small peptide fragment.
%
\chap~\ref{ch:drosophila} then discusses using manifold learning to extract developmental dynamics from a set of fixed images.
%
Here, we use vector diffusion maps (an extension of diffusion maps) to both register and temporally order experimental imaging data sets in developmental biology.
%
\chap~\ref{ch:harmonics} discusses the issue of ``repeated eigendirections'' in manifold learning, where subsequent modes can be higher harmonics of previous modes and parameterize the same direction in the data set.
%
We present an algorithm, based on local linear regression, for automatically detecting these repeated eigendirections which allows us to uncover the most parsimonious representation of the data and deduce the true dimensionality of the underlying manifold, an essential task when constructing a reduced model.
%
\chap~\ref{ch:conclusion} then summarizes this work and offers some future potential research directions for using and adapting manifold learning techniques to analyze data from dynamical systems. 